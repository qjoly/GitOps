{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome aboard! This is a collection of notes and guides that I have written to help me remember how to do things. I hope you find them useful too. Keep in mind that everything here is a work in progress and that it's mostly for personal use, so it might not be the best way to do things, however, many choices are made with ease of use in mind. First things first, let's explain what a CloudLab is. What the heck is a CloudLab? In opposition to a HomeLab, a CloudLab a server rented from a cloud provider. It's a great way to experiment with new technologies without the need to buy hardware and deal with the noise and heat of the servers. In my personal case, I have both a Homelab and a CloudLab. I use the CloudLab to test new technologies and the Homelab to host sensitive data and services (clusters home is the name of my Homelab cluster, cortado and arabica are CloudLab clusters). I have 2 n100 servers at home, both are running Proxmox and are connected to a 1G switch as well as my NAS.s Sorry cable management enthusiasts, I'm not there yet. Which technologies are used? Like many people in the k8s-at-home community, I use Kubernetes to manage my services and prioritize Helm charts to deploy them. But many services / applications doesn't have a Helm chart (or the one provided doesn't respect my requirements) so I chose to write my own templates (If you start looking at my configuration, you will sometimes see common-charts instead of the official Helm chart). To install and managed my Kubernetes cluster, I use Talos , a modern OS for Kubernetes. It's a great way to have a minimal OS that is easy to manage and secure. On top of that, I also have Omni to manage my cluster. Omni allows me to manage many clusters from a single interface and provides features like gRPC Proxy (to access the Kubernetes API of all clusters, directly from Omni and by using a standard Kubeconfig, e.g. here ), RBAC, Deployments, and more. All Omni templates are stored in this repository (e.g. here ) and are applied with omnictl . Here are just a few of the technologies and applications I use : - Omni (Self-hosted) : Manage all nodes between clusters and regions. - Cilium as CNI and LB (ARP mode) - ArgoCD to manage the GitOps workflow - Nginx Ingress Controller for Ingress management (and Istio deployed on some clusters) - Cert Manager for TLS certificates. - Longhorn for storage based on nodes disks ( Only on the home cluster ). - External Secrets to fetch secrets from a remote store. - Vault as a secret store to store secrets. - Cloudflare Tunnels to expose services to the internet ( Only on the home cluster ). - ZFS + Local-Path-Provisioner to create persistent volumes on the mounted ZFS filesystem ( Only on CloudLab cluster ). - Volsync to create backup and send backup (using restic) to a minio server ( Only on CloudLab cluster ).","title":"Welcome aboard!"},{"location":"#welcome-aboard","text":"This is a collection of notes and guides that I have written to help me remember how to do things. I hope you find them useful too. Keep in mind that everything here is a work in progress and that it's mostly for personal use, so it might not be the best way to do things, however, many choices are made with ease of use in mind. First things first, let's explain what a CloudLab is.","title":"Welcome aboard!"},{"location":"#what-the-heck-is-a-cloudlab","text":"In opposition to a HomeLab, a CloudLab a server rented from a cloud provider. It's a great way to experiment with new technologies without the need to buy hardware and deal with the noise and heat of the servers. In my personal case, I have both a Homelab and a CloudLab. I use the CloudLab to test new technologies and the Homelab to host sensitive data and services (clusters home is the name of my Homelab cluster, cortado and arabica are CloudLab clusters). I have 2 n100 servers at home, both are running Proxmox and are connected to a 1G switch as well as my NAS.s Sorry cable management enthusiasts, I'm not there yet.","title":"What the heck is a CloudLab?"},{"location":"#which-technologies-are-used","text":"Like many people in the k8s-at-home community, I use Kubernetes to manage my services and prioritize Helm charts to deploy them. But many services / applications doesn't have a Helm chart (or the one provided doesn't respect my requirements) so I chose to write my own templates (If you start looking at my configuration, you will sometimes see common-charts instead of the official Helm chart). To install and managed my Kubernetes cluster, I use Talos , a modern OS for Kubernetes. It's a great way to have a minimal OS that is easy to manage and secure. On top of that, I also have Omni to manage my cluster. Omni allows me to manage many clusters from a single interface and provides features like gRPC Proxy (to access the Kubernetes API of all clusters, directly from Omni and by using a standard Kubeconfig, e.g. here ), RBAC, Deployments, and more. All Omni templates are stored in this repository (e.g. here ) and are applied with omnictl . Here are just a few of the technologies and applications I use : - Omni (Self-hosted) : Manage all nodes between clusters and regions. - Cilium as CNI and LB (ARP mode) - ArgoCD to manage the GitOps workflow - Nginx Ingress Controller for Ingress management (and Istio deployed on some clusters) - Cert Manager for TLS certificates. - Longhorn for storage based on nodes disks ( Only on the home cluster ). - External Secrets to fetch secrets from a remote store. - Vault as a secret store to store secrets. - Cloudflare Tunnels to expose services to the internet ( Only on the home cluster ). - ZFS + Local-Path-Provisioner to create persistent volumes on the mounted ZFS filesystem ( Only on CloudLab cluster ). - Volsync to create backup and send backup (using restic) to a minio server ( Only on CloudLab cluster ).","title":"Which technologies are used?"},{"location":"backup/","text":"Backup system When I created the cortado cluster (which is a single node cluster), I used a ZFS zpool (created manually) and local-path-provisionner to create persistent volumes on the mounted ZFS filesystem. To backup the data, I use volsync to create backup and send backup (using restic) to a minio server. I will have a restic repository per backup source. Create a restic repository export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY export RESTIC_PASSWORD=REPOSITORY_PASSWORD export RESTIC_REPOSITORY=s3:http://MINIO_URL:9000/BUCKET From your workstation, create the repository restic init Then, create a secret in the cluster with the credentials to access the repository kubectl create secret generic -n kube-system restic-credentials \\ --from-literal=AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ --from-literal=AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ --from-literal=RESTIC_PASSWORD=$RESTIC_PASSWORD \\ --from-literal=RESTIC_REPOSITORY=$RESTIC_REPOSITORY Use an ExternalSecret apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: restic-credentials namespace: komga spec: refreshInterval: \"30s\" secretStoreRef: name: vault-backend kind: ClusterSecretStore target: name: restic-credentials data: - secretKey: AWS_ACCESS_KEY_ID remoteRef: key: restic property: AWS_ACCESS_KEY_ID - secretKey: AWS_SECRET_ACCESS_KEY remoteRef: key: restic property: AWS_SECRET_ACCESS_KEY - secretKey: RESTIC_PASSWORD remoteRef: key: restic property: RESTIC_PASSWORD - secretKey: RESTIC_REPOSITORY remoteRef: key: restic property: RESTIC_REPOSITORY You can now create a ReplicationSource to backup a specific PVC. apiVersion: volsync.backube/v1alpha1 kind: ReplicationSource metadata: name: komga spec: # The PVC name to backup sourcePVC: komga-data trigger: schedule: \"*/5 * * * *\" restic: pruneIntervalDays: 7 repository: restic-credentials retain: hourly: 6 daily: 5 weekly: 4 monthly: 2 yearly: 1 copyMethod: Direct From your workstation, you can see backups $ restic snapshots repository f8c8acc2 opened (version 2, compression level auto) created new cache in /Users/qjoly/Library/Caches/restic ID Time Host Tags Paths Size ----------------------------------------------------------------------- 7ebbffdd 2024-12-24 09:38:48 volsync /data 1.689 MiB 8a7574a4 2024-12-24 09:55:06 volsync /data 1.689 MiB 9b5b7a1a 2024-12-24 10:00:07 volsync /data 1.689 MiB ----------------------------------------------------------------------- 3 snapshots Restore a backup Eh, eh, this is still a work in progress. I will update this section when I have more information.","title":"Backup"},{"location":"backup/#backup-system","text":"When I created the cortado cluster (which is a single node cluster), I used a ZFS zpool (created manually) and local-path-provisionner to create persistent volumes on the mounted ZFS filesystem. To backup the data, I use volsync to create backup and send backup (using restic) to a minio server. I will have a restic repository per backup source.","title":"Backup system"},{"location":"backup/#create-a-restic-repository","text":"export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY export RESTIC_PASSWORD=REPOSITORY_PASSWORD export RESTIC_REPOSITORY=s3:http://MINIO_URL:9000/BUCKET From your workstation, create the repository restic init Then, create a secret in the cluster with the credentials to access the repository kubectl create secret generic -n kube-system restic-credentials \\ --from-literal=AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ --from-literal=AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ --from-literal=RESTIC_PASSWORD=$RESTIC_PASSWORD \\ --from-literal=RESTIC_REPOSITORY=$RESTIC_REPOSITORY Use an ExternalSecret apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: restic-credentials namespace: komga spec: refreshInterval: \"30s\" secretStoreRef: name: vault-backend kind: ClusterSecretStore target: name: restic-credentials data: - secretKey: AWS_ACCESS_KEY_ID remoteRef: key: restic property: AWS_ACCESS_KEY_ID - secretKey: AWS_SECRET_ACCESS_KEY remoteRef: key: restic property: AWS_SECRET_ACCESS_KEY - secretKey: RESTIC_PASSWORD remoteRef: key: restic property: RESTIC_PASSWORD - secretKey: RESTIC_REPOSITORY remoteRef: key: restic property: RESTIC_REPOSITORY You can now create a ReplicationSource to backup a specific PVC. apiVersion: volsync.backube/v1alpha1 kind: ReplicationSource metadata: name: komga spec: # The PVC name to backup sourcePVC: komga-data trigger: schedule: \"*/5 * * * *\" restic: pruneIntervalDays: 7 repository: restic-credentials retain: hourly: 6 daily: 5 weekly: 4 monthly: 2 yearly: 1 copyMethod: Direct From your workstation, you can see backups $ restic snapshots repository f8c8acc2 opened (version 2, compression level auto) created new cache in /Users/qjoly/Library/Caches/restic ID Time Host Tags Paths Size ----------------------------------------------------------------------- 7ebbffdd 2024-12-24 09:38:48 volsync /data 1.689 MiB 8a7574a4 2024-12-24 09:55:06 volsync /data 1.689 MiB 9b5b7a1a 2024-12-24 10:00:07 volsync /data 1.689 MiB ----------------------------------------------------------------------- 3 snapshots","title":"Create a restic repository"},{"location":"backup/#restore-a-backup","text":"Eh, eh, this is still a work in progress. I will update this section when I have more information.","title":"Restore a backup"},{"location":"cloudflared/","text":"Expose applications securely with Cloudflare Tunnel To avoid exposing services directly to the internet, we use Cloudflare Tunnel to securely expose them on few clusters (e.g. home that can't be accessed directly since port 80 and 443 are already used by another service). Note that this is free in the Cloudflare plan, you only need to have a domain name managed by Cloudflare. Cloudflare Install cloudflared, this is the client that will create the tunnel between the cluster and Cloudflare. You can still use the web interface to create the tunnel, but it's easier to manage it with the CLI. brew install cloudflared cloudflared login # Login to your Cloudflare account Create a tunnel with the CLI, note the ID of the tunnel that will be used later. cloudflared tunnel create home-cluster Tunnel credentials written to /Users/qjoly/.cloudflared/2f3b093d-bd57-4708-8d54-42723da21338.json. cloudflared chose this file based on where your origin certificate was found. Keep this file secret. To revoke these credentials, delete the tunnel. Created tunnel home-cluster with id 2f3b093d-bd57-4708-8d54-42723da21338 By creating the tunnel, you obtain a JSON file with the credentials that will be used to authenticate the tunnel with Cloudflare, keep it safe and do not commit it to the repository (or encrypt it if you do, even if it's still not recommended). kubectl create namespace cloudflare kubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=/Users/qjoly/.cloudflared/2f3b093d-bd57-4708-8d54-42723da21338.json \\ --namespace=cloudflare Install the Cloudflare Tunnel Controller in the cloudflare namespace. values.yaml cloudflare: tunnelName: \"home-cluster\" tunnelId: \"2f3b093d-bd57-4708-8d54-42723da21338\" secretName: \"tunnel-credentials\" ingress: - hostname: \"*.ur-domain.com\" # Change the service name depending on the ingress controller used service: \"https://ingress-nginx-controller.kube-system.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 helm repo add cloudflare https://cloudflare.github.io/helm-charts helm repo update helm upgrade --install cloudflare-tunnel cloudflare/cloudflare-tunnel \\ --namespace cloudflare \\ --values values.yaml ArgoCD Application apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cloudflare-tunnel namespace: argocd spec: project: default source: repoURL: https://cloudflare.github.io/helm-charts chart: cloudflare-tunnel targetRevision: 0.3.2 helm: values: | cloudflare: tunnelName: \"home-cluster\" tunnelId: \"2f3b093d-bd57-4708-8d54-42723da21338\" secretName: \"cloudflare-tunnel\" ingress: - hostname: \"*.thoughtless.eu\" service: \"https://ingress-nginx-controller.ingress-nginx.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 destination: server: https://kubernetes.default.svc namespace: cloudflare syncPolicy: automated: prune: true syncOptions: - CreateNamespace=true If you create a CNAME record in Cloudflare pointing to <tunnelId>.cfargotunnel.com , you can access the services on the cluster using the domain name (e.g. mine is 2f3b093d-bd57-4708-8d54-42723da21338.cfargotunnel.com ) External DNS To automatically create DNS records in Cloudflare, we use External DNS. This will create a DNS record for each Ingress resource created in the cluster. Create an API key in Cloudflare that can edit your DNS records of the domain (Zone) you configured in the Cloudflare kubectl create ns external-dns cat <<EOF | kubectl apply -f - apiVersion: v1 stringData: token: ${CF_API_TOKEN} kind: Secret metadata: name: cloudflare-api-key namespace: external-dns type: Opaque EOF Then install External DNS with Helm. helm repo add kubernetes-sigs https://kubernetes-sigs.github.io/external-dns/ helm repo update helm upgrade --install external-dns kubernetes-sigs/external-dns \\ --namespace external-dns \\ --set sources[0]=ingress \\ --set policy=sync \\ --set provider.name=cloudflare \\ --set env[0].name=CF_API_TOKEN \\ --set env[0].valueFrom.secretKeyRef.name=cloudflare-api-key \\ --set env[0].valueFrom.secretKeyRef.key=apiKey Now, you can add annotations to your Ingress resources to automatically create DNS records in Cloudflare. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" external-dns.alpha.kubernetes.io/hostname: nginx.thoughtless.eu external-dns.alpha.kubernetes.io/target: 2f3b093d-bd57-4708-8d54-42723da21338.cfargotunnel.com name: nginx namespace: default spec: ingressClassName: nginx rules: - host: nginx.thoughtless.eu http: paths: - backend: service: name: nginx port: number: 80 path: / pathType: Prefix ArgoCD Application --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: external-dns namespace: argocd annotations: # Application is needed before the secret argocd.argoproj.io/sync-wave: \"-1\" spec: project: default source: repoURL: https://kubernetes-sigs.github.io/external-dns/ chart: external-dns targetRevision: 1.15.0 helm: parameters: - name: sources[0] value: ingress - name: policy value: sync - name: provider.name value: cloudflare - name: env[0].name value: CF_API_TOKEN - name: env[0].valueFrom.secretKeyRef.name value: cloudflare-api-key - name: env[0].valueFrom.secretKeyRef.key value: apiKey destination: server: https://kubernetes.default.svc namespace: external-dns syncPolicy: automated: prune: true syncOptions: - CreateNamespace=true","title":"Cloudflared"},{"location":"cloudflared/#expose-applications-securely-with-cloudflare-tunnel","text":"To avoid exposing services directly to the internet, we use Cloudflare Tunnel to securely expose them on few clusters (e.g. home that can't be accessed directly since port 80 and 443 are already used by another service). Note that this is free in the Cloudflare plan, you only need to have a domain name managed by Cloudflare.","title":"Expose applications securely with Cloudflare Tunnel"},{"location":"cloudflared/#cloudflare","text":"Install cloudflared, this is the client that will create the tunnel between the cluster and Cloudflare. You can still use the web interface to create the tunnel, but it's easier to manage it with the CLI. brew install cloudflared cloudflared login # Login to your Cloudflare account Create a tunnel with the CLI, note the ID of the tunnel that will be used later. cloudflared tunnel create home-cluster Tunnel credentials written to /Users/qjoly/.cloudflared/2f3b093d-bd57-4708-8d54-42723da21338.json. cloudflared chose this file based on where your origin certificate was found. Keep this file secret. To revoke these credentials, delete the tunnel. Created tunnel home-cluster with id 2f3b093d-bd57-4708-8d54-42723da21338 By creating the tunnel, you obtain a JSON file with the credentials that will be used to authenticate the tunnel with Cloudflare, keep it safe and do not commit it to the repository (or encrypt it if you do, even if it's still not recommended). kubectl create namespace cloudflare kubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=/Users/qjoly/.cloudflared/2f3b093d-bd57-4708-8d54-42723da21338.json \\ --namespace=cloudflare Install the Cloudflare Tunnel Controller in the cloudflare namespace. values.yaml cloudflare: tunnelName: \"home-cluster\" tunnelId: \"2f3b093d-bd57-4708-8d54-42723da21338\" secretName: \"tunnel-credentials\" ingress: - hostname: \"*.ur-domain.com\" # Change the service name depending on the ingress controller used service: \"https://ingress-nginx-controller.kube-system.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 helm repo add cloudflare https://cloudflare.github.io/helm-charts helm repo update helm upgrade --install cloudflare-tunnel cloudflare/cloudflare-tunnel \\ --namespace cloudflare \\ --values values.yaml ArgoCD Application apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cloudflare-tunnel namespace: argocd spec: project: default source: repoURL: https://cloudflare.github.io/helm-charts chart: cloudflare-tunnel targetRevision: 0.3.2 helm: values: | cloudflare: tunnelName: \"home-cluster\" tunnelId: \"2f3b093d-bd57-4708-8d54-42723da21338\" secretName: \"cloudflare-tunnel\" ingress: - hostname: \"*.thoughtless.eu\" service: \"https://ingress-nginx-controller.ingress-nginx.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 destination: server: https://kubernetes.default.svc namespace: cloudflare syncPolicy: automated: prune: true syncOptions: - CreateNamespace=true If you create a CNAME record in Cloudflare pointing to <tunnelId>.cfargotunnel.com , you can access the services on the cluster using the domain name (e.g. mine is 2f3b093d-bd57-4708-8d54-42723da21338.cfargotunnel.com )","title":"Cloudflare"},{"location":"cloudflared/#external-dns","text":"To automatically create DNS records in Cloudflare, we use External DNS. This will create a DNS record for each Ingress resource created in the cluster. Create an API key in Cloudflare that can edit your DNS records of the domain (Zone) you configured in the Cloudflare kubectl create ns external-dns cat <<EOF | kubectl apply -f - apiVersion: v1 stringData: token: ${CF_API_TOKEN} kind: Secret metadata: name: cloudflare-api-key namespace: external-dns type: Opaque EOF Then install External DNS with Helm. helm repo add kubernetes-sigs https://kubernetes-sigs.github.io/external-dns/ helm repo update helm upgrade --install external-dns kubernetes-sigs/external-dns \\ --namespace external-dns \\ --set sources[0]=ingress \\ --set policy=sync \\ --set provider.name=cloudflare \\ --set env[0].name=CF_API_TOKEN \\ --set env[0].valueFrom.secretKeyRef.name=cloudflare-api-key \\ --set env[0].valueFrom.secretKeyRef.key=apiKey Now, you can add annotations to your Ingress resources to automatically create DNS records in Cloudflare. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" external-dns.alpha.kubernetes.io/hostname: nginx.thoughtless.eu external-dns.alpha.kubernetes.io/target: 2f3b093d-bd57-4708-8d54-42723da21338.cfargotunnel.com name: nginx namespace: default spec: ingressClassName: nginx rules: - host: nginx.thoughtless.eu http: paths: - backend: service: name: nginx port: number: 80 path: / pathType: Prefix ArgoCD Application --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: external-dns namespace: argocd annotations: # Application is needed before the secret argocd.argoproj.io/sync-wave: \"-1\" spec: project: default source: repoURL: https://kubernetes-sigs.github.io/external-dns/ chart: external-dns targetRevision: 1.15.0 helm: parameters: - name: sources[0] value: ingress - name: policy value: sync - name: provider.name value: cloudflare - name: env[0].name value: CF_API_TOKEN - name: env[0].valueFrom.secretKeyRef.name value: cloudflare-api-key - name: env[0].valueFrom.secretKeyRef.key value: apiKey destination: server: https://kubernetes.default.svc namespace: external-dns syncPolicy: automated: prune: true syncOptions: - CreateNamespace=true","title":"External DNS"},{"location":"getting-started/","text":"Getting Started Location In most cases, I suggest you to host your services at home. It's cheaper and you have more control over your data. However, if you don't have a good internet connection or you don't want to deal with the noise and heat of the servers, you can rent a server from a cloud provider. I experimented with a few cloud providers and I found that OVH is the best in terms of price and performance. They have a data center in multiple locations (Europe, North America, Asia) and provide many features like private network, Terraform support, and a good API. Keep in mind that I'm not affiliated with OVH and that we're in the case of a personal project. Let's see how to install our baremetal cluster on OVH. I didn't test vRack (private network between multiple servers) with OVH. If used it, please let me know how it went :smile: . OVH Install Talos OVH provides many ways to install an OS on your server (Templates, Bring-your-own-image ) but I will use the Rescue Mode to install Talos. After rebooting in Rescue Mode, you will receive an email with the credentials (or just the IP if you provided an SSH Key) to connect to the server. You can connect to the server using SSH. Once connected, you can download the Talos image and install it on the server. If you're using Omni, you can download the image with the following command: omnictl download iso --arch amd64 --extensions zfs --talos-version v1.9.1 # ZFS is optional, but I use it to create a ZFS pool, see more in ./docs/zfs.md Then, you can use dd to write the image to the system disk. dd if=talos.iso of=/dev/sda bs=4M status=progress 26+1 records in 26+1 records out 112099328 bytes (112 MB, 107 MiB) copied, 1.50875 s, 74.3 MB/s For now, Talos doesn't handle RAID . After writing the image to the disk, you can reboot the server and boot on the disk. You should have a Talos node running on your server !","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#location","text":"In most cases, I suggest you to host your services at home. It's cheaper and you have more control over your data. However, if you don't have a good internet connection or you don't want to deal with the noise and heat of the servers, you can rent a server from a cloud provider. I experimented with a few cloud providers and I found that OVH is the best in terms of price and performance. They have a data center in multiple locations (Europe, North America, Asia) and provide many features like private network, Terraform support, and a good API. Keep in mind that I'm not affiliated with OVH and that we're in the case of a personal project. Let's see how to install our baremetal cluster on OVH. I didn't test vRack (private network between multiple servers) with OVH. If used it, please let me know how it went :smile: .","title":"Location"},{"location":"getting-started/#ovh-install-talos","text":"OVH provides many ways to install an OS on your server (Templates, Bring-your-own-image ) but I will use the Rescue Mode to install Talos. After rebooting in Rescue Mode, you will receive an email with the credentials (or just the IP if you provided an SSH Key) to connect to the server. You can connect to the server using SSH. Once connected, you can download the Talos image and install it on the server. If you're using Omni, you can download the image with the following command: omnictl download iso --arch amd64 --extensions zfs --talos-version v1.9.1 # ZFS is optional, but I use it to create a ZFS pool, see more in ./docs/zfs.md Then, you can use dd to write the image to the system disk. dd if=talos.iso of=/dev/sda bs=4M status=progress 26+1 records in 26+1 records out 112099328 bytes (112 MB, 107 MiB) copied, 1.50875 s, 74.3 MB/s For now, Talos doesn't handle RAID . After writing the image to the disk, you can reboot the server and boot on the disk. You should have a Talos node running on your server !","title":"OVH Install Talos"},{"location":"secret-management/","text":"Secret Management To avoid storing secrets in the repository, we use Vault to store them (e.g., API keys, passwords, etc.). We then use the External Secrets operator to sync the secrets stored in Vault with the Kubernetes cluster. Vault Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. Vault provides a unified interface to any secret while providing tight access control and recording a detailed audit log. It is installed in the vault namespace using the ArgoCD application vault (see here for an example). Initialize the Vault cluster Everytime we restart the Vault cluster, we need to unseal it. The unseal process requires a quorum of the keys to be entered. In a normal condition, we would have multiple keys and multiple people to enter them. In our case, we have only one key and we will use it to unseal the cluster (maybe not the best practice, but it's a lab and I can't handle auto unseal for now). kubectl exec -n vault vault-0 -- vault operator init \\ -key-shares=1 \\ -key-threshold=1 \\ -format=json > cluster-keys.json You obtain the root token in the cluster-keys.json file. Ensure to keep it safe and do not commit it to the repository (or encrypt it if you do). age -R ~/.ssh/id_ed25519.pub cluster-keys.json > cluster-keys.json.age # Encrypt the file age -d -i ~/.ssh/id_ed25519 cluster-keys.json.age > cluster-keys.json # Decrypt the file Now that our first Vault server is initialized, we need to unseal it and ask all the other Vault to join the cluster. export VAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json) kubectl exec -n vault -ti vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY kubectl exec -n vault -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 kubectl exec -n vault -ti vault-1 -- vault operator unseal $VAULT_UNSEAL_KEY kubectl exec -n vault -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 kubectl exec -n vault -ti vault-2 -- vault operator unseal $VAULT_UNSEAL_KEY If you restart the Vault cluster, you will need to unseal it again by running the same command. Enable the KV secret engine Once the Vault cluster is ready, we can enable the KV secret engine kv (I will store the secrets in the kv path). kubectl port-forward -n vault svc/vault 8200:8200 export VAULT_ADDR=\"http://localhost:8200\" export VAULT_TOKEN=$(jq -r \".root_token\" cluster-keys.json) vault secrets enable kv vault login $(jq -r \".root_token\" cluster-keys.json) To test the Vault cluster, we can write a secret in the KV secret engine. vault kv put kv/foo my-value=s3cr3t External Secrets External Secrets allows you to use secrets stored in external secret management systems like AWS Secrets Manager, GCP Secret Manager, Azure Key Vault, and HashiCorp Vault. It is installed in the external-secrets namespace using the ArgoCD application external-secrets (see here for an example). Note: This is actually the root token, which is not recommended for production use. In a production environment, you should create a dedicated token with the appropriate policies. Create a secret with the Vault token cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret stringData: token: $(jq -r \".root_token\" cluster-keys.json) metadata: name: vault-token namespace: external-secrets type: Opaque EOF Create a secret store that points to the Vault server cat <<EOF | kubectl apply -f - apiVersion: external-secrets.io/v1beta1 kind: ClusterSecretStore metadata: name: vault-backend spec: provider: vault: server: \"http://vault.vault.svc.cluster.local:8200\" path: \"kv\" version: \"v1\" auth: tokenSecretRef: name: \"vault-token\" key: \"token\" namespace: \"external-secrets\" EOF Test the External Secrets Create an External Secret that syncs the secret in Vault with the Kubernetes cluster cat <<EOF | kubectl apply -f - apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: vault-example namespace: default spec: refreshInterval: \"15s\" secretStoreRef: name: vault-backend kind: ClusterSecretStore target: name: example-sync # Name of the Secret in the target namespace data: - secretKey: foobar remoteRef: key: foo property: my-value EOF ArgoCD Vault Plugin For multiple applications, I use the ArgoCD Vault Plugin to sync the secrets stored in Vault with the Kubernetes cluster. The reason behind this is that some data I want to hide are not in secrets and ConfigMap (.e.g. I want to use a vault secret in an deployment annotation). Create a secret with the Vault token for the Vault Plugin cat <<EOF | kubectl apply -f - apiVersion: v1 stringData: AVP_AUTH_TYPE: token AVP_KV_VERSION: \"1\" AVP_TYPE: vault VAULT_ADDR: http://vault.vault.svc.cluster.local:8200 VAULT_TOKEN: $(jq -r \".root_token\" cluster-keys.json) kind: Secret metadata: name: vault-credentials namespace: argocd type: Opaque EOF Now, we need to reinstall the ArgoCD but with the Vault Plugin enabled. cd ./common/argocd/vault-argocd/ kubectl apply -k . -n argocd Once everything is applied, we would be able to use the Vault Plugin in the ArgoCD application (e.g. here I want to hide the domain used by the ingress) apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: glance namespace: argocd spec: project: default syncPolicy: syncOptions: - CreateNamespace=true destination: server: https://kubernetes.default.svc namespace: glance source: repoURL: https://rubxkube.github.io/charts/ chart: glance targetRevision: 0.0.2 plugin: name: argocd-vault-plugin-helm env: - name: HELM_VALUES value: | common: ingress: enabled: true hostName: \"glance.<path:kv/cluster#domain>\" # Just here ! ingressClassName: nginx","title":"Secret Management"},{"location":"secret-management/#secret-management","text":"To avoid storing secrets in the repository, we use Vault to store them (e.g., API keys, passwords, etc.). We then use the External Secrets operator to sync the secrets stored in Vault with the Kubernetes cluster.","title":"Secret Management"},{"location":"secret-management/#vault","text":"Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. Vault provides a unified interface to any secret while providing tight access control and recording a detailed audit log. It is installed in the vault namespace using the ArgoCD application vault (see here for an example).","title":"Vault"},{"location":"secret-management/#initialize-the-vault-cluster","text":"Everytime we restart the Vault cluster, we need to unseal it. The unseal process requires a quorum of the keys to be entered. In a normal condition, we would have multiple keys and multiple people to enter them. In our case, we have only one key and we will use it to unseal the cluster (maybe not the best practice, but it's a lab and I can't handle auto unseal for now). kubectl exec -n vault vault-0 -- vault operator init \\ -key-shares=1 \\ -key-threshold=1 \\ -format=json > cluster-keys.json You obtain the root token in the cluster-keys.json file. Ensure to keep it safe and do not commit it to the repository (or encrypt it if you do). age -R ~/.ssh/id_ed25519.pub cluster-keys.json > cluster-keys.json.age # Encrypt the file age -d -i ~/.ssh/id_ed25519 cluster-keys.json.age > cluster-keys.json # Decrypt the file Now that our first Vault server is initialized, we need to unseal it and ask all the other Vault to join the cluster. export VAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json) kubectl exec -n vault -ti vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY kubectl exec -n vault -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 kubectl exec -n vault -ti vault-1 -- vault operator unseal $VAULT_UNSEAL_KEY kubectl exec -n vault -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 kubectl exec -n vault -ti vault-2 -- vault operator unseal $VAULT_UNSEAL_KEY If you restart the Vault cluster, you will need to unseal it again by running the same command.","title":"Initialize the Vault cluster"},{"location":"secret-management/#enable-the-kv-secret-engine","text":"Once the Vault cluster is ready, we can enable the KV secret engine kv (I will store the secrets in the kv path). kubectl port-forward -n vault svc/vault 8200:8200 export VAULT_ADDR=\"http://localhost:8200\" export VAULT_TOKEN=$(jq -r \".root_token\" cluster-keys.json) vault secrets enable kv vault login $(jq -r \".root_token\" cluster-keys.json) To test the Vault cluster, we can write a secret in the KV secret engine. vault kv put kv/foo my-value=s3cr3t","title":"Enable the KV secret engine"},{"location":"secret-management/#external-secrets","text":"External Secrets allows you to use secrets stored in external secret management systems like AWS Secrets Manager, GCP Secret Manager, Azure Key Vault, and HashiCorp Vault. It is installed in the external-secrets namespace using the ArgoCD application external-secrets (see here for an example). Note: This is actually the root token, which is not recommended for production use. In a production environment, you should create a dedicated token with the appropriate policies. Create a secret with the Vault token cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret stringData: token: $(jq -r \".root_token\" cluster-keys.json) metadata: name: vault-token namespace: external-secrets type: Opaque EOF Create a secret store that points to the Vault server cat <<EOF | kubectl apply -f - apiVersion: external-secrets.io/v1beta1 kind: ClusterSecretStore metadata: name: vault-backend spec: provider: vault: server: \"http://vault.vault.svc.cluster.local:8200\" path: \"kv\" version: \"v1\" auth: tokenSecretRef: name: \"vault-token\" key: \"token\" namespace: \"external-secrets\" EOF","title":"External Secrets"},{"location":"secret-management/#test-the-external-secrets","text":"Create an External Secret that syncs the secret in Vault with the Kubernetes cluster cat <<EOF | kubectl apply -f - apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: vault-example namespace: default spec: refreshInterval: \"15s\" secretStoreRef: name: vault-backend kind: ClusterSecretStore target: name: example-sync # Name of the Secret in the target namespace data: - secretKey: foobar remoteRef: key: foo property: my-value EOF","title":"Test the External Secrets"},{"location":"secret-management/#argocd-vault-plugin","text":"For multiple applications, I use the ArgoCD Vault Plugin to sync the secrets stored in Vault with the Kubernetes cluster. The reason behind this is that some data I want to hide are not in secrets and ConfigMap (.e.g. I want to use a vault secret in an deployment annotation). Create a secret with the Vault token for the Vault Plugin cat <<EOF | kubectl apply -f - apiVersion: v1 stringData: AVP_AUTH_TYPE: token AVP_KV_VERSION: \"1\" AVP_TYPE: vault VAULT_ADDR: http://vault.vault.svc.cluster.local:8200 VAULT_TOKEN: $(jq -r \".root_token\" cluster-keys.json) kind: Secret metadata: name: vault-credentials namespace: argocd type: Opaque EOF Now, we need to reinstall the ArgoCD but with the Vault Plugin enabled. cd ./common/argocd/vault-argocd/ kubectl apply -k . -n argocd Once everything is applied, we would be able to use the Vault Plugin in the ArgoCD application (e.g. here I want to hide the domain used by the ingress) apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: glance namespace: argocd spec: project: default syncPolicy: syncOptions: - CreateNamespace=true destination: server: https://kubernetes.default.svc namespace: glance source: repoURL: https://rubxkube.github.io/charts/ chart: glance targetRevision: 0.0.2 plugin: name: argocd-vault-plugin-helm env: - name: HELM_VALUES value: | common: ingress: enabled: true hostName: \"glance.<path:kv/cluster#domain>\" # Just here ! ingressClassName: nginx","title":"ArgoCD Vault Plugin"},{"location":"zfs/","text":"ZFS Since Talos doesn't support RAID yet, I use ZFS to create a RAIDZ2 pool with 3 disks. Once the pool is created, I use local-path-provisionner to create persistent volumes on the mounted ZFS filesystem. When I installed Talos on my server, I specified the zfs extension to download the Talos image with ZFS support. I also had to enable the ZFS module in the kernel. Here is the patch to enable the ZFS module in the kernel: machine: kernel: modules: - name: zfs Once Talos rebooted, I run a debug pod to create the ZFS pool: kubectl -n kube-system debug -it --profile sysadmin --image=alpine node/{your-node} To create the ZFS pool, we need to get the ID of the disk ( since name could change, we use the ID to identify the disk). We can use the ls command to list the disks: / # ls -lh /host/dev/disk/by-id/ total 0 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H3U3ZA -> ../../sdc lrwxrwxrwx 1 root root 10 Dec 17 20:34 ata-HGST_HUS726020ALA610_K5H3U3ZA-part1 -> ../../sdc1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H3U3ZA-part9 -> ../../sdc9 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H64MYA -> ../../sdd lrwxrwxrwx 1 root root 10 Dec 17 20:34 ata-HGST_HUS726020ALA610_K5H64MYA-part1 -> ../../sdd1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H64MYA-part9 -> ../../sdd9 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HM8S6A -> ../../sdb lrwxrwxrwx 1 root root 10 Dec 17 20:34 ata-HGST_HUS726020ALA610_K5HM8S6A-part1 -> ../../sdb1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HM8S6A-part9 -> ../../sdb9 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA -> ../../sda lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part1 -> ../../sda1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part2 -> ../../sda2 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part3 -> ../../sda3 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part4 -> ../../sda4 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part5 -> ../../sda5 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part6 -> ../../sda6 Before creating the pool, we need to clean the disks by using the wipefs command, please be careful, this command will erase all the data on the disk: apk add wipefs wipefs /dev/sdb --all --force wipefs /dev/sdc --all --force wipefs /dev/sdd --all --force apk add xfsprogs mkfs.xfs /dev/sdb mkfs.xfs /dev/sdc mkfs.xfs /dev/sdd Then, we can create the pool: chroot /host zpool create -f \\ -o ashift=12 \\ -O mountpoint=\"/var/zfs_pool\" \\ -O xattr=sa \\ -O compression=zstd \\ -O acltype=posixacl \\ -O atime=off \\ hdd \\ raidz2 \\ /dev/disk/by-id/ata-HGST_HUS726020ALA610_K5H3U3ZA \\ /dev/disk/by-id/ata-HGST_HUS726020ALA610_K5H64MYA \\ /dev/disk/by-id/ata-HGST_HUS726020ALA610_K5HM8S6A Once the pool is created, we can deploy the local-path-provisionner: # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - github.com/rancher/local-path-provisioner/deploy?ref=v0.0.26 patches: - patch: |- kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: config.json: |- { \"nodePathMap\":[ { \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\", \"paths\":[\"/var/zfs_pool\"] } ] } - patch: |- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-path annotations: storageclass.kubernetes.io/is-default-class: \"true\" - patch: |- apiVersion: v1 kind: Namespace metadata: name: local-path-storage labels: pod-security.kubernetes.io/enforce: privileged You should now be able to generate PVC on the ZFS pool.","title":"ZFS"},{"location":"zfs/#zfs","text":"Since Talos doesn't support RAID yet, I use ZFS to create a RAIDZ2 pool with 3 disks. Once the pool is created, I use local-path-provisionner to create persistent volumes on the mounted ZFS filesystem. When I installed Talos on my server, I specified the zfs extension to download the Talos image with ZFS support. I also had to enable the ZFS module in the kernel. Here is the patch to enable the ZFS module in the kernel: machine: kernel: modules: - name: zfs Once Talos rebooted, I run a debug pod to create the ZFS pool: kubectl -n kube-system debug -it --profile sysadmin --image=alpine node/{your-node} To create the ZFS pool, we need to get the ID of the disk ( since name could change, we use the ID to identify the disk). We can use the ls command to list the disks: / # ls -lh /host/dev/disk/by-id/ total 0 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H3U3ZA -> ../../sdc lrwxrwxrwx 1 root root 10 Dec 17 20:34 ata-HGST_HUS726020ALA610_K5H3U3ZA-part1 -> ../../sdc1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H3U3ZA-part9 -> ../../sdc9 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H64MYA -> ../../sdd lrwxrwxrwx 1 root root 10 Dec 17 20:34 ata-HGST_HUS726020ALA610_K5H64MYA-part1 -> ../../sdd1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5H64MYA-part9 -> ../../sdd9 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HM8S6A -> ../../sdb lrwxrwxrwx 1 root root 10 Dec 17 20:34 ata-HGST_HUS726020ALA610_K5HM8S6A-part1 -> ../../sdb1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HM8S6A-part9 -> ../../sdb9 lrwxrwxrwx 1 root root 9 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA -> ../../sda lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part1 -> ../../sda1 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part2 -> ../../sda2 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part3 -> ../../sda3 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part4 -> ../../sda4 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part5 -> ../../sda5 lrwxrwxrwx 1 root root 10 Dec 17 20:33 ata-HGST_HUS726020ALA610_K5HPPLTA-part6 -> ../../sda6 Before creating the pool, we need to clean the disks by using the wipefs command, please be careful, this command will erase all the data on the disk: apk add wipefs wipefs /dev/sdb --all --force wipefs /dev/sdc --all --force wipefs /dev/sdd --all --force apk add xfsprogs mkfs.xfs /dev/sdb mkfs.xfs /dev/sdc mkfs.xfs /dev/sdd Then, we can create the pool: chroot /host zpool create -f \\ -o ashift=12 \\ -O mountpoint=\"/var/zfs_pool\" \\ -O xattr=sa \\ -O compression=zstd \\ -O acltype=posixacl \\ -O atime=off \\ hdd \\ raidz2 \\ /dev/disk/by-id/ata-HGST_HUS726020ALA610_K5H3U3ZA \\ /dev/disk/by-id/ata-HGST_HUS726020ALA610_K5H64MYA \\ /dev/disk/by-id/ata-HGST_HUS726020ALA610_K5HM8S6A Once the pool is created, we can deploy the local-path-provisionner: # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - github.com/rancher/local-path-provisioner/deploy?ref=v0.0.26 patches: - patch: |- kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: config.json: |- { \"nodePathMap\":[ { \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\", \"paths\":[\"/var/zfs_pool\"] } ] } - patch: |- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-path annotations: storageclass.kubernetes.io/is-default-class: \"true\" - patch: |- apiVersion: v1 kind: Namespace metadata: name: local-path-storage labels: pod-security.kubernetes.io/enforce: privileged You should now be able to generate PVC on the ZFS pool.","title":"ZFS"}]}